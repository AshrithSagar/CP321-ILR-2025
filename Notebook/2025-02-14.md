# ILR | 2025-02-14

## Learning stable DS controller

- Want to learn a dynamical system
  - Want a learner that doesn't replan but rather is reactive to changes in the environment during the motion planning task
  - ~Just need to do real time inference, which is faster
- Model

$$
\dot x = f(x) \longrightarrow \Big\{ \lim_{t \to \infty} \Vert x - x^* \Vert = 0, \qquad x, \dot x \in \mathbb{R}^n, \quad f: \mathbb{R}^n \to \mathbb{R}^n
$$

where $x^*$ is the target position, and $x$ being the position of the end-effector

- LASA dataset: Handwriting motions
  - Data: Set of $M$ reference trajectories

$$
\{ X, \dot X \} = \Bigg\{ \Big\{ x_t, \dot x_t \Big\}_{t=1}^{T_m} \Bigg\}_{m=1}^{M}
$$

- $T_m$ is the length of each trajectory

- In machine learning, there is a very standard assumption to draw training and test samples being i.i.d.
  - The test data points being close to the training data points yield predictable results
  - Standard machine learning attempts to learn using this *behavioral cloning*
- Issues with these approaches
  - Diverge in the regions of the state space where no data is collected
  - ~ Does not guarentee convergence
- Resources
  - <https://wensun.github.io/CS4789_data/Imitation_Learning_April_8_annotated.pdf>

## Lyapunov theory for stable DS

### Global asymptotic stability (GAS)

$$
V(x^*) = 0
$$

---

- Lyapunov function $V(x)$ closely resembles like an energy function.

### Stability conditions for LTI DS

Given the system
$$
\dot x = f(x) = A x + b
$$
and the Lyapunov function
$$
V(x) = \frac{1}{2} {(x - x^*)}^\top (x - x^*)
$$
The linear DS is GAS at the attractor $x^*$ if $A$ is negative definite.

#### Proof

$$
\implies \dot V(x) = 2 \frac{1}{2} {(x - x^*)}^\top \dot x = {(x - x^*)}^\top (A x + b)
$$

For $x^*$, we have
$$
\dot x^* = 0 \implies A x^* + b = 0 \implies b = - A x^*
$$

$$
\implies \dot V(x) = {(x - x^*)}^\top (A x - A x^*) = {(x - x^*)}^\top A (x - x^*)
$$

Thereby to have $\dot V(x) < 0 \ \forall x \neq 0$, we need $A$ to be negative definite.

---

To check if $A$ is negative definite

- Take the eigenvalues of the symmetric part of $A$, which should all be ~negative.

### Stability conditions for a mixture of LTI DS

Seen in LWR & GMR, previously.

Given the system
$$
\dot x = f(x) = \sum_{k=1}^{K} \gamma_k(x) \left( A^k x + b^k \right)
$$

- ~ $A^k, b^k$, the exponents are just identifiers for the system (?)

The Lyapunov function
$$
V(x) = \frac{1}{2} {(x - x^*)}^\top (x - x^*)
$$
The non-linear DS is GAS at the attractor $x^*$ if

- $b^k = - A^k x^*$ $\longrightarrow$ Stability of attractor
- $A^k + {(A^k)}^\top \ll 0$ $\longrightarrow$

#### Proof

$$
\implies \dot V(x) = 2 \frac{1}{2} {(x - x^*)}^\top \dot x = {(x - x^*)}^\top \sum_{k=1}^{K} \gamma_k(x) \left( A^k x + b^k \right)
$$

For $x^*$, we have
$$
\dot x^* = 0 \implies \sum_{k=1}^{K} \gamma_k(x) \left( A^k x^* + b^k \right) = 0
$$
The terms $\gamma_k \geq 0$, and then somehow (?), we then get $b^k = - A^k x^*$

## SEDS: Stable Estimator of Dynamical Systems

Gaussian mixture regression can be written in the form
$$
\dot x = \sum_{k=1}^{K} \gamma_k(x) \left( A^k x + b^k \right)
$$
Mixing function $\gamma_k(x)$

- Summary
  - An extension of Gaussian mixture model

- ~Starts from where EM algorithm left

Limitations

- Non-convex optimisation

Later on, closed loop analysis of parameters ensuring end-to-end convergence were developed, etc

References

- <https://www.youtube.com/watch?v=MZVFq-65AAc&ab_channel=LASA>
- <https://infoscience.epfl.ch/entities/publication/3b2c1ce5-b0d7-49d7-974a-922d24cf0938>
- <https://ieeexplore.ieee.org/document/5953529>
- <https://cs.stanford.edu/people/khansari/DSMotions.html>

---

