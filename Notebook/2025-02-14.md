# ILR | 2025-02-14

## Learning stable DS controller

- Want to learn a dynamical system
  - Want a learner that doesn't replan but rather is reactive to changes in the environment during the motion planning task
- Model

$$
\dot x = f(x) \longrightarrow \Big\{ \lim_{t \to \infty} \Vert x - x^* \Vert = 0, \qquad x, \dot x \in \mathbb{R}^n, \quad f: \mathbb{R}^n \to \mathbb{R}^n
$$

where $x^*$ is the target position, and $x$ being the position of the end-effector

- LASA dataset: Handwriting motions
  - Data: Set of $M$ reference trajectories

$$
\{ X, \dot X \} = \Bigg\{ \Big\{ x_t, \dot x_t \Big\}_{t=1}^{T_m} \Bigg\}_{m=1}^{M}
$$

- $T_m$ is the length of each trajectory

- In machine learning, there is a very standard assumption to draw training and test samples being i.i.d.
  - The test data points being close to the training data points yield predictable results
  - Standard machine learning attempts to learn using this *behavioral cloning*
- Issues with these approaches
  - Diverge in the regions of the state space where no data is collected
  - ~ Does not guarentee convergence
- Resources
  - <https://wensun.github.io/CS4789_data/Imitation_Learning_April_8_annotated.pdf>

### Lyapunov theory for stable DS

#### Global asymptotic stability (GAS)

$$
V(x^*) = 0
$$

---

- Lyapunov function $V(x)$ closely resembles like an energy function.

#### Stability conditions for LTI DS



