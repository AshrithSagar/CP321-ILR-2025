# ILR | 2025-02-26

- SEDS
  - Needs multiple trajectories
  - ~ Involves an inversion, covariance matrix
  - ~Generalisable to goal points

- DMP
  - Temporal scaling
  - Guarenteed convergence
  - One trajectory is enough?
  - ~Generalisable to goal points
- ProMP
  - Allows modulating trajectories not only at the goal but anywhere
  - ~Generalisable to goal as well as via points
  - No stability guarentees

- (?)
  - Alleoteric
  - Epistemic $\to$ reducible


## TP-GMM: Task parameterised GMM movement primitives

- Quadratic costs minimisation as a product of Gaussians

  $$
  c(x) = (x - \mu)^\top W (x - \mu)
  $$

  where $W$ is a constant matrix. Since

  $$
  \frac{\partial x^\top A x}{\partial x} = 2 A x
  $$

  $$
  \implies \frac{\partial c(x)}{\partial x} = 2 W (x - \mu) \implies x = \mu
  $$

- Now, with $K$ cost functions

  $$
  \min_{x} \sum_{k = 1}^{K} (x - \mu_k)^\top W_k (x - \mu_k)
  $$

  Taking derivatives w.r.t. $x$ to minimise

  $$
  \implies \sum_{k = 1}^{K} 2 W_k (x - \mu_k) = 0 \implies \left( \sum_{k = 1}^{K} W_k \right) x = \left( \sum_{k = 1}^{K} W_k \mu_k \right)
  $$

  $$
  \implies x^+ = {\left( \sum_{k = 1}^{K} W_k \right)}^{-1} \left( \sum_{k = 1}^{K} W_k \mu_k \right)
  $$

  - Through Frobenius norms (?)

- Note: Completion of squares
  $$
  X^\top X + X^\top b = {\left( X + \frac{1}{2} A^{-1} b \right)}^{-1} A \left( X + \frac{1}{2} A^{-1} b \right) - \frac{1}{4} b^\top A^{-1} b
  $$

- Product of Gaussians
  - Improving numerical stability, for $\widehat \Sigma$

- TP-GMM

  - Reproduction of inertial frames (?)

  - Want to find an optimal trajectory, from multiple viewpoints of a same trajectory

  - A global frame, and different local frames

  - $\mathcal{N}(\xi, \Sigma)$ in the global frame

  - $\mathcal{N}(\xi_k, \Sigma_k)$ in the $k$-th local frame

  - $P$ frames

  - $K$ is the number of Gaussian to ~model the data with (?)

  - ~Transformations relating the projections (?)
    $$
    \begin{aligned}
    \widehat \xi_k^j & = A_j \ \xi_k + b_j \\
    \widehat \Sigma_k^j & = A_j \Sigma_k A_k^\top
    \end{aligned}
    $$
    
  - $\widehat \xi^j, \widehat \Sigma^j$ are in the global frame

    $$
    p(\widehat \xi) = \sum_{k = 1}^{K} \pi_k \ \mathcal{N}(\widehat \xi \mid \widehat \mu_k, \widehat \Sigma_k)
    $$

- Training by EM algorithm
- Choosing the frames as some key points; Eg: On the end-effector, say

- In previous methods, orientation wasn't captured
- By using frames, orientation is inherently captured in TP-GMM
- ~ Statistical learning
- ~ Idea that changing frames globally, doesn't change distribution much (?)

- ~ Out-of-distribution predictions (?)

---

