# ILR | 2025-01-29

- Topics
  - Non-linear least squares
  - Linear estimation
  - Linear least squares
  - Weighted least squares
  - Locally weighted regression
  - Bayesian

## Radial basis function network (RBFN)

- $$
  f(\mathbf{x}) = \sum_{e = 1}^{E} \mathbf{w}_e \cdot \phi(\mathbf{x}, \mathbf{c}_e)
  $$

- Feature matrix $\Phi(\mathbf{x}, \mathbf{x})$

  - $$
    \mathbf{\Phi}(\mathbf{x}, \mathbf{x})
    =
    \begin{bmatrix}
    \phi(\mathbf{x}_1, \mathbf{x}_1) & \phi(\mathbf{x}_1, \mathbf{x}_2) & \cdots & \phi(\mathbf{x}_1, \mathbf{x}_n)
    \\
    \phi(\mathbf{x}_2, \mathbf{x}_1) & \phi(\mathbf{x}_2, \mathbf{x}_2) & \cdots & \phi(\mathbf{x}_2, \mathbf{x}_n)
    \\
    \vdots & \vdots & \ddots & \vdots
    \\
    \phi(\mathbf{x}_n, \mathbf{x}_1) & \phi(\mathbf{x}_n, \mathbf{x}_2) & \cdots & \phi(\mathbf{x}_n, \mathbf{x}_n)
    \end{bmatrix}
    $$

- Least square solution

  - $$
    \mathbf{w}^* = {(\boldsymbol{\Phi}^\top \boldsymbol{\Phi})}^{-1} \boldsymbol{\Phi}^\top \mathbf{y} = \boldsymbol{\Phi}^+ \mathbf{y}
    $$

## Kernel ridge regression (KRR)

- https://teazrq.github.io/SMLR/kernel-ridge-regression.html

- Non-parametric method

- Every data point is a centre of a basis function

- $$
  f(\mathbf{x}) = \sum_{n=1}^{N} \mathbf{w}_n \cdot k(\mathbf{x}, \mathbf{x}_n)
  $$

- Gram matrix $\mathbf{K}(\mathbf{x}, \mathbf{x})$

  - $$
    \mathbf{K}(\mathbf{x}, \mathbf{x})
    =
    \begin{bmatrix}
    k(\mathbf{x}_1, \mathbf{x}_1) & k(\mathbf{x}_1, \mathbf{x}_2) & \cdots & k(\mathbf{x}_1, \mathbf{x}_n)
    \\
    k(\mathbf{x}_2, \mathbf{x}_1) & k(\mathbf{x}_2, \mathbf{x}_2) & \cdots & k(\mathbf{x}_2, \mathbf{x}_n)
    \\
    \vdots & \vdots & \ddots & \vdots
    \\
    k(\mathbf{x}_n, \mathbf{x}_1) & k(\mathbf{x}_n, \mathbf{x}_2) & \cdots & k(\mathbf{x}_n, \mathbf{x}_n)
    \end{bmatrix}
    $$

- Least square solution

  - $$
    \mathbf{w}^*
    = {(\mathbf{K}^\top \mathbf{K})}^{-1} \mathbf{K}^\top \mathbf{y}
    = \mathbf{K}^{+} \mathbf{y}
    $$

- 

## Mixture of Gaussians

### Gaussian mixture regression (GMR)

- Mixture of linear models

- Gaussian distribution

  - $$
    \mathcal{N}(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi}} ...
    $$

- Multivariate Gaussian distribution

  - $$
    \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \frac{1}{{(2\pi)}^{D/2}} \frac{1}{{\vert \boldsymbol{\Sigma} \vert}^{1/2}} \exp \left\{ -\frac{1}{2} {(\mathbf{x} - \boldsymbol{\mu})}^\top \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}) \right\}
    $$

- In the case of independent samples, it becomes

  - $$
    \frac{1}{2 \pi} \prod_{i=1}^{n} ...
    $$

- Log-likelihood function

  - $$
    \ln p(\mathbf{X} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}) = - \frac{ND}{2} \ln(2\pi) - \frac{N}{2} \ln \vert \boldsymbol{\Sigma} \vert - \frac{1}{2} \sum_{n=1}^{N} (\mathbf{X}_n - \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{X}_n - \boldsymbol{\mu})
    $$

- Maximum likelihood for the mixture of Gaussians

  - $$
    \ln p(\mathbf{X} \mid \boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \sum_{n=1}^{N} \ln{\sum_{k=1}^{K} \pi_k \; \mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}
    $$

  - MLE estimates

- Maximising likelihood OR Minimising negative log-likelihood

- Using logarithms

  - Probability values are small, multiplying a lot of them might cause diminishing problems
    - Numerical instability, Underflows
  - Converting multiplication to addition
  - Monotone
  - Easy derivatives

#### EM algorithm

#### Cross validation

#### Akaike information criterion (AIC)

- https://en.wikipedia.org/wiki/Akaike_information_criterion

#### Bayesian information criterion (BIC)

- https://en.wikipedia.org/wiki/Bayesian_information_criterion


### Gaussian mixture model (GMM)

- Unsupervised

### Gaussian process regression (GPR)

- ~Continuous version?

---

